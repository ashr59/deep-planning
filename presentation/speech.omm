Welcome to my thesis defense, Learning Abstract Features from Images and Audio with stacked denoising autoencoders. I'll be showing you some experiments where I've taken SDAs trained on images, and switched them to audio to see how they perform, and vice versa.

-- Motivation --
The mammalian neocortex performs an incredible service to the rest of the brain. It builds a model of both sensory inflow and motor outflow produced by other parts of the brain. It learns a general model of "what happens" in very short term - to the very long term, and eventually overrides our observations and our instincts with predictions of it's own. The neocortex is a generalized pattern finder and controller, which can and will learn to make the best of anything you throw at it. It's uniform structure and the speed which with select areas adapt to new tasks suggests that it is using a similar process throughout it's extent.

Connect a robotic arm to the cortex of a monkey, and it will be assimilated, as if it were the monkey's own limb. 

Place an array of electrodes on your tongue, connect them to camera, and you can learn to see without your eyes. 

Wire up the auditory nerve from a rat's ear to it's primary visual cortex, and it can learn to hear with a part of its cortex that did not evolve for hearing. 

We need to find this algorithm. I believe we will find it, and deep networks are very close. We are going to find out whatever the cortex is doing, replicate it, and turn it up to eleven.

That's the motive for learning about stacked denoising autoencoders. In this thesis presentation I'll show you two experiments which tell us something about the plasticity of stacked denoising autoencoders. I'll go ahead and tell you up front, I didn't find what I expected, but that happens in science. 

While I say we are close, Human level intelligence is still a really far-off goal, There's also no good consensus on how to define intelligence, although there are some useful definitions out there. What I find more useful in the short term are the idiosyncracies of the brain that help us frame the problem.

For example, our way of perceiving things mixes together our wishes, and our observations, giving us a biased model of the world. The behavior of existing unsupervised learning algorithms suggest that this is not a flaw, but a fundamental part of perception. 

Another idiosyncracy is that we dream. For reasons poorly understood, our brain needs to periodically change modes. This may be an insightful design constriant which leads us in the right direction, or it may be a dead end, but at least it's an easier constraint to meet than "having intelligence"

In this thesis, I am considering the plasticity of the neocortex with regards to it's behavior when switched from one sensory modality to another to be an idiosyncracy worth studying. I believe that a learning algorithm which is mediocre at a variety of tasks is more brain-like than a specialized system with near human-level performance on only one task. 

-- Experiment Design --

I'm using an unsupervised learning algorithm called stacked denoising autoencoders, recently improved upon and brought into the spotlight by Geoffrey Hinton and Yoshua Bengio.

The SDA can be understood in three parts, I'll talk about them in the reverse order they are referred to in the acronym: Autoencoder, Denoising, and Stacked. 

An autoencoder is a classical three-layer neural network which learns the identity function. this just means that it is given a vector on its input and the weights are optimized to produce the same vector on the output. The reason this is non-trivial and useful is that the number of hidden nodes is less than the size of the input and output, so the autoencoder must perform some lossy compression. Alternatively, more hidden nodes can be used if a sparsity term is used in the cost function.

A "denoising" autoencoder is a slight improvement on an autoencoder that generalizes better. By randomly setting some of the values in each input vector to zero, it reconstructs corrupted versions of training examples. To do this, it must learn the correlations that exist between the values in the input distribution.

Finally, A stacked denoising autoencoder, is a stack of denoising autoencoders where each one takes as input the activations of the hidden nodes from the one below.

The innovation found by Geoff Hinton that catapulted this algorithm to the front of the machine vision field was that if you train the layers one at a time, you can create a deep network that has converged on a much better solution than you could ever get with a classical neural network of the same topology.

Now that I've covered the algorithm I'll be using, I'll explain how I'm going to explore it's plasticity. I prepared two datasets of identical size and dimensionality. One of images from flickr, and one of audio from NPR and several college radio stations. I wanted to observe the effects of changing the dataset from one sensory modality to the other mid-training.

I set up 8 different experiments to fully explore the combinations of three different binary variables. These are: interleaved or layer-wise training, single-modal or multi-modal, and audio or images as the test data. 

Interleaved vs. layer-wise training is the difference between training the individual DAs in an SDA like this {000...111...222} vs. like this {012...012...012}. Since layer-wise training is known to help the higher layers of SDAs converge on better features, I hypothesised that it would improve adaptability in the case of multi-modal data as well. 

Single-modal vs. multi modal refers to whether I used the same data thru-out the experiment or swapped out the dataset half-way. the term "modal" comes from the term "sensory modality"

Finally, to control for the advantage given by training order, I run multi modal experiments with audio first, and images first. I run single-modal experiments with either just audio or just images. 

Each of these 8 experiments is run 20 times, using random seeds, to obtain a distribution of results. I compare the recontruction error on the test data from each dataset to isolate the effect of multi-modal, and layer-wise training.

-- Results and Interpretation --
Here are the reconstruction error distributions for each experiment. Better performance is to the left. By comparing these values, we can learn whether layer-wise training, the supposed magic bullet of deep networks, has an effect on the plasticity of SDAs. In other words, do the layer-wise multi-modal experiments (highlight) have a lower reconstruction error than their interleaved counterparts. (highlight, indicate association) An improved (smaller) reconstruction error would indicate that the model is sucsessfully adapting to the new sensory modality.

The answer is that yes, it did help a little bit for both modality orders. The distribution for experiment 4 lies entirely to the left of experiment 2, and the distribution for experiment 8 lies not entirely, but signifigantly to the left of experiment 6. (highlight or point)

There are some unexpected differences though. The error distributions for models trained on audio only are way over here. Something wen't wrong with those models, and in the next few slides I'll show some graphs that explain that a little better, but the truth is I don't know why they do that, all I can say is that it reliably happens every time and we're talking about the same code that worked fine on images. If the audio-only models were like the image-only models, we would expect them to be here (exp. 5, 3.0) and here (exp. 7, 2.8).

Another way to get an idea of what your network has learned is to visualize the features in each layer. These are the hypothetical input examples that would maximally activate each node. This gives an approximation of what the node is sensitive to.

In this visualization of the interleaved image-only model, the 1st layer's 256 hidden nodes are on the left, the second layer's 64 hidden nodes are in the middle, and the third layer's 16 hidden nodes are at the bottom right. Note what fraction of the available nodes have converged on a smooth, spatially correlated pattern, vs. nodes which still display uniformly distributed noise. Also, note how the smoothness, the size, and the redundancy of the features changes from layer one to three.

In the next model, interleaved audio-only, we get a clue as to why the reconstruction error for audio only models was worse than expected. Layer two has not converged on spatially correlated patterns. But even more strangely, layer three has. At first I looked at this and thought, "that's impossible, something is broken."  But there is nothing in the cost function of the interleaved training method that demands spatially correlated patterns in individual layers (only layer-wise training demands that) the interleaved training method only asks for a decent reconstruction from the whole whole network. Additionally, the weakness of this visualization is that it does not give a clear idea of what a node is coding for, only what the hypothetical input that would produce an activation of 1 for that node and an activation of 0 for all others in the same layer, a condition that is unlikely to ever happen in practice.

In the next two slides we can see layer-wise pre-training doing its job: enforcing every layer to converge on a sparse distributed representation of the one below.  The resulting features are more noisy, but they also allow the network to generalize better and thus achieve better reconstruction error on the test set. 

Back on the audio dataset, layer-wise pre-training has helped with the lack of spatially correlated patterns in layer two, but it still displays a strange tendency for that second layer to be noisier than usual.