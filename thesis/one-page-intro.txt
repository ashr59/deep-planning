Applying Stacked De-noising Autoencoders to Planning
(with a focus on short-term vs. long-term solutions)
Nathan Nifong


Stacked de-noising autoencoders are a type of deep belief network that can discover arbitrarily deeply nested hierarchical structure in data, loosely based on a hypothetical organization of mammalian neocortex. Geoffrey Hinton et al. have shown that adding another layer to the stack of autoencoders always increases the upper bound on prediction accuracy in machine perception tasks. Often, each additional layer will model a more abstract representation of the layer below it. For example, when a stacked de-noising autoencoder (SDA) is applied to a machine vision task, the first layer autoencoder may compress the input by reducing it to a collection of edges, solid colors, and spots of various colors and orientations. See Figure 1 for an example of the features that a single layer de-noising auto-encoder produces from the MINST handwritten digit dataset.  Once that layer is trained, another layer can be added above it, which takes the compressed representation (the activations of the hidden nodes) and uses them as input. It will eventually find a more compressed representation of that space. It can converge faster than the first layer because it is operating on a smaller, and smoother space. The basic building block, a de-nosing autoencoder, is a neural network that learns the identity function for given data in a space. It is a traditional three layer neural network, with an input layer, a hidden layer, and an output layer, with the special condition that the output layer is the same size as the input layer, and the same activation of the input is expected on the output and used in the backpropagation algorithm to train the weights. Either using fewer hidden nodes than input nodes, or enforcing sparsity by using some measure of it to introduce a penalty in the objective function causes the network to learn a compressed representation of the input space. It is essentially reconstructing the input data from an encoding with fewer bits.  Autoencoders are a type of content addressable memory. If you lack part of a signal, but have an autoencoder trained on a class of such signals, it will produce an estimate of a typical signal from that class that is close to the one provided, or reconstruct the data. It will fill in the blanks, much like the brain, or specifically, the neocortex. Autoencoders can be stacked to improve their prediction accuracy. Once a single level auto-encoder is trained, it's hidden node activation levels are used as the input space for another autoencoder. Opon first discovery of this algorithm, by (who?) the performance was unimpressive, but with the addition of the crucial de-noising modification, these stacks of autoencoders become capable of modeling very complex data, and show comparable performance to other, well established deep belief networks like . . . . . . . . . . .

These successes have only been demonstrated in the field of machine perception however. And we know that the neocortex's responsibilities are both perception and action, or at least planning action, and sending the plans to the cerebellum. So can stacked-denoising autoencoders perform well in planning tasks? especially in ones which have the same nested levels of abstraction as a typical natural image or sound has? What would be the planning task equivalent of perceptual levels of abstraction? first, you may plan to go to the kitchen and get something to eat. this consists of several sub-actions which must be performed in series and in paralell, and those sub-actions also consist of sub-actions. But how can an algorithm learn that structure without having a huge dataset of well planned out actions to study? Note that some of the sub-actions of going to the kitchen to get something to eat involve perception. There are undetermined forks that cannot be planned out from the beginning, like what exactly will you take from the refridgerator? I think that planning and perception are the same process. The process of expecting likely and desirable observations, and then taking any actions you would expect your body to take in those situations. You learned most of the expectations, from accidently fooling around and noticing patterns. And through a bunch of mimicry, and language that is beyond the scope of this simple modelling excersize. Pretend we are trying to make a cat brain.

Perception is like nested stack of expectations, specific expectations of the near future based on a deluge of sensory data, and a smaller collection of abstract expectations of the medium range future based on a shower of specific, short-range expectations and some more sensory data. Recurse until lost.

Does this tree have a root? is there some "most abstract representation" of our sensory experience that we can expect in the indefinitely long run, that decodes to the specifics that we will in fact observe from this point on? I think that this very theory is a candidate. But that's not as important as it sounds. What any abstract expectation actually predicts depends on the more specific models below it, and the sensory data yet to be observed.  "This too shall pass" would make a perfectly good root for our tree, if giving it that role didn't actually turn it into a paradox. A root here is really not more important than a leaf.

--put this somewhere--
Interestingly, an SDA trained to see, can more quickly learn to hear than one that has had no training at all, and vice-versa. (who?)

Let me re-iterate. I am operating on the assumtion for this project, that perception and planning are both part of the same process. The model of sensory data is influenced by three main factors. The accurcy, the sparsity, and the desirability, or pleasurableness to the creature. In the case of a game this last factor would be the points or wins/losses experienced by the player. The player percieves the state of the game and the state of it's own recent actions, and expects a likely, sparse, and desirable scenario next, and then is made to take any actions it expected of itself which are legal in the rules of the game. It may not be the most brilliant way to play a game, but I think It would at least perform OK at Puerto Rico, a game about economic growth.

Jeff hawkins said in 2004, "Neural networks were a genuine improvement of the AI approach because their architecture is based, though very loosely, on real nervous systems...But as the neural network pehenomennon exploded on the scene, it mostly settled on the class of ultrasimple models that didn't meet any of [his criteria for biological plausibility]. Most neural networks consisted of a small number of neurons connected in three rows...I thought the field would qucikly move on to more realistic networks, but it didn't. Because these simple neural networks were able to do interesting things, research seemed to stop there for years."

Stacked denoising autoencoders are more complex nerural networks, who's basic component are a variant of those simple, 50-year-old, three-layer networks. The are still simple compared to the neocortex, but have some of the same performance characteristics. The neocortex has several layers, but those layers do not correspond to the layers of the stack in a stacked denoising neural autoencoder. An SDA only models the topmost layer and the lateral connections between areas of the cortex. For example, the areas V1 through V4 in the visual cortex are organized in a functional heirarchy, but they are not in a physical stack of layers. These confusing overlaps of terminology are cleared up somewhat in the following animation.

..... scrap this whole paragraph's
Stacked de-noising auto-encoders, and other deep belief networks show much promise in machine perception, they are relatively new, and have a wide range of applicability to AI problems. They may not be an optimal allocation of computational resources in all cases but they show surprisingly human qualities. I want to apply them to planning, because they are powerful and could perform well. Some planning tasks require a sustainable outlook, and many traditional planning algorithms are too greedy. Additionally, in games which end, and for games with an infinite payoff for the winner, which is effectively like ending, and in games in which you can change the rules, a strategic balance between short term and long term solutions must be found. You know how a sustainable strategy can beat a greedy one, but alternatively, a player who uses an exploitative short term tactic can beat a player who sticks to sustainable strategies, because he may win, or change the rules of the game before the time comes to suffer the consequences. An agent who can chain together short periods of exploitation punctuated by changes to the game's rules can achieve an impossibly high rate of growth to an agent who simply simply acts sustainably within the current rules. This type of play would require generating nested plans, and in order to navigate this vasty space, the levels of planning would need to be segmented into say, high level strategy, and low level tactics. A deep belief network planning by "optimistic dreaming", as I describe below, would structure it's model of the possible actions in the game in just this way. Just to be clear, it would not be possible to literally "change the rules" but sub-systems within the DBN may be operating in a sub-game invented by the DBN with more restricted rules than the super-game the whole system is playing.

How
The task of planning in AI, or strategy building in game theory is a difficult one. The usual way to do it is to search and rank possible futures within computational limits. Planning with a generative deep belief net is simpler. All the work is done during perception, and generating a plan takes about the same amount of time no matter how complex the problem. Generating likely sequences of percepts, or "dreaming" as DBN researchers often call it, is the inverse of perception. All that is required to make a plan is to generate a likely sequence of events using a network that has preferentially remembers sequences of that led to desirable outcomes. This is what I am calling "planning by optimistic dreaming." I will test a planning system that uses a stack of de-noising auto-encoders in this way by allowing it to play a game where there are appropriate risks and rewards and a balance between short and long term solutions is optimal. My current choice is the board game "Puerto Rico" which has themes of sustainability and growth. I plan to encode the rules of Puerto Rico or whatever game I eventually decide to use, into my existing open source networked, turn-based game arbitration software, which I previously used in a Mancala AI competition. I will be building off of the deeplearning.net suite of Python tools to implement the stacked de-noising encoder, and running the system on my personal GPU cluster, if I can get that to work. I will be branching off from the term project I worked on for Will Landecker's advanced machine learning class, so I have some code and results already. I expect the planning task to be computationally simpler than the typical machine perception tasks to which this algorithm is usually applied, so I shouldn't run up against any hard physical limits. I'll be spending about half of my time on writing, and half on coding. In my opinion, publishing freely available, well-documented, running code on the Internet, provides the highest value to the machine learning and systems science communities.


Collected Resources

http://jmlr.csail.mit.edu/papers/volume11/vincent10a/vincent10a.pdf
This is the paper that introduces the critical de-noising step that gives stacked de-noising autoencoders their strength. 

http://deeplearning.net/tutorial/SdA.html
This is a hands-on introduction to stacked de-noising autoencoders provided by the excellent deeplearning.net group using their high quality python package.

http://www.youtube.com/watch?v=VdIURAu1-aU&t=5m23s
This is a link to an instant in a youtube video where Geoffrey Hinton describes how his student discovered that adding another level to the stack of autoencoders always improved the performance. Turn on the interactive transcript to read along. I'm currently searching for the paper in which this claim is proved. The whole tech talk is awesome and I highly recommend watching it from the beginning if you have the time.

Hinton, G. E., Osindero, S., & Teh, Y. (2006). A fast learning algorithm for deep belief nets. Neural Computation, 18, 1527–1554.