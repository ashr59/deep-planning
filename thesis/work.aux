\relax 
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Abstract}{1}}
\newlabel{Abstract}{{0.1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {0.2}Introduction}{1}}
\newlabel{Introduction}{{0.2}{1}}
\citation{Bengio09}
\citation{Vincint08}
\citation{Hinton06}
\@writefile{toc}{\contentsline {section}{\numberline {0.3}0.1 Stacked de-noising autoencoders}{3}}
\newlabel{Stacked de-noising autoencoders}{{0.3}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An SdA with two autoencoders. Each autoencoder is a a simple three layer network. The input/output to the first layer is the pink level, and the yellow level is it's distributed code, or hidden layer. The yellow hidden layer is then used as input/output to another autoencoder, it's hidden layer is shown in green.}}{4}}
\citation{Le12}
\citation{Le12}
\citation{Le12}
\citation{Bergstra10}
\@writefile{toc}{\contentsline {section}{\numberline {0.4}1.0 Successes of SdAs on machine perception tasks}{5}}
\newlabel{Successes of SdAs on machine perception tasks}{{0.4}{5}}
\citation{Baker95}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A representation of the face detection feature learned by Google's SdA. This image is generated by computing the input that would maximally activate the hidden node in the top layer that has come to represent faces. The SdA was trained on 10 million 200x200 images from youtube\cite  {Le12}.}}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {0.5}1.1 Applications of SdAs to planning tasks}{6}}
\newlabel{Applications of SdAs to planning tasks}{{0.5}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {0.6}2.0 Plausibility of SdA's as a model of the mammalian neocortex}{7}}
\newlabel{Plausibility of SdA's as a model of the mammalian neocortex}{{0.6}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {0.7}2.1 Philosophical points}{8}}
\newlabel{Philosophical points}{{0.7}{8}}
\citation{Hawkins04}
\@writefile{toc}{\contentsline {section}{\numberline {0.8}3.0 Expected performance of SdA vs greedy approach}{9}}
\newlabel{Expected performance of SdA vs. dA}{{0.8}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {0.9}3.1 Implementation of Go Engine}{10}}
\newlabel{Implementation of Go Engine}{{0.9}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {0.10}3.2 Training the SdA}{11}}
\newlabel{Training the SdA}{{0.10}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {0.11}4.0 Performance of SdA-based player against dA player and various other opponents}{11}}
\newlabel{Performance of SdA-based player against dA player and various other opponents}{{0.11}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A representation of the go position that would maximally activate one of the hidden units from the highest level in the SdA. Stones are either colored white or black based on the direction of the prediction, and the opacity is the confidence that a stone would be in that position rather than being empty.}}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {0.12}4.1 Conclusion and lessons learned}{12}}
\newlabel{Conclusion and lessons learned}{{0.12}{12}}
\bibcite{Baker95}{1}
\bibcite{Bengio07}{2}
\bibcite{Bengio09}{3}
\bibcite{Hawkins04}{4}
\bibcite{Hinton06}{5}
\bibcite{Hinton07}{6}
\bibcite{Bergstra10}{7}
\bibcite{Le12}{8}
\bibcite{Ranzato06}{9}
\bibcite{Seyfarth01}{10}
\bibcite{Vincint08}{11}
