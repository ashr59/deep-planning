\documentclass[12pt]{article}
\usepackage{titlesec}
\usepackage{setspace}
\usepackage{graphicx} 
\usepackage[left=1.5in,top=1in,right=1in,bottom=1in]{geometry}
\textwidth 6in
\textheight 9in
\parskip 7.2pt           % sets spacing between paragraphs
\parindent 0pt		 % sets leading space for paragraphs
\titleformat{\section}{\bfseries}{\thesection}{12pt}{}
\newcommand{\sectionbreak}{\clearpage}
\begin{document}         
% Start your text

\section{Abstract}
\label{Abstract}
\doublespacing

One of the most impressive qualities of the brain is it's plasticity. The neocortex has roughly the same structure throughout it's whole surface, yet it is involved in a variety of different tasks from vision to motor control, and regions which once performed one task, can learn to perform another. Any machine learning algorithm which claims to be a plausible model of the neocortex should also display this plasticity. One such candidate is the stacked de-noising autoencoder (SdA) It has shown promising results in the field of machine perception where it has been used to learn abstract features from unlabeled data. In this thesis I develop a flexible distributed implementation of an SdA and test it on simple tasks to investigate it's plasticity.

\title{Application of Stacked De-Noising Auto-Encoders to Planning}
\section{Criteria for a hierarchical associative memory}

I was inspired by several researchers with ideas about the brain. Jeff Hawkins, who argues that although the neocortex has many specialized regions, that they all learn using roughly the same process, and Daniel Wolpert, who stresses that the evolutionary purpose of the brain is to produce complex adaptive movements. These scientists, and many others have been inching closer to a model which meets the many criteria for a plausible model of the neocortex. To be more specific, at this point, it appears we are looking for an online associative memory which can learn arbitrarily deep hierarchies of abstraction with the following criteria:

1. Growable - The model must be composed of a large number of similar units, which can be grown outward according to a simple pattern to increase the size of the total network. These units could be analogous to neurons, cortical columns, or some larger structure. This is desirable because it simplifies the process by which the neocortex could be grown in animals, and in the machine learning community, it is practical, easy to understand, and economical. We know that cortices of varying sizes occur in many mammals, and that the cortex provides some adaptive benefit at any size.

2. General - The model must be sufficiently general that it could be applied to video, audio, tactile, somatosensory, and other more abstract, data with minimal tuning of metaparameters for each kind of data. To measure plasticity, one might train a model on some images, and then, train it on some audio and compare it's predictive performance to a randomly initialized model of equal size trained on the same audio for the same number of epochs.

3. Online - The model must learn and predict continuously and incorporate new data as it becomes available. Most researchers prefer to train model on batches of data, and then test them, partly because offline algorithms are simpler, and it affords scientifically desirable properties like repeatability, but I think that the further down the road of offline learning algorithms we go, the more our results will be irrelevant to a plausible model of the neocortex. Often however, online learning algorithms are simpler, for example, the on-line and off-line versions of k-means.

4. Capable of motor control - The neocortex is involved in motor control. It's not fully in charge of motor control, but it's definitely involved, and we must reconcile how an associative memory could be part of a motor control and planning system. Can motor control be formulated as a categorically similar problem to perception? Such as in Bill Power's Perceptual Control Theory which hypothesizes that perceptions are controlled rather than actions, and that action is the result of negative feedback systems bringing the world into agreement with perception. I suggest that motor control is somehow unified with perception, such that the same process which can abstract faces and cars from lines and spots in visual cortex, could abstract waving or waterskiing from postures and cycles of motion. It could presumably use the same information pathways by which predictions are propagated downward towards the senses, to decode actions into muscle contractions. However, it may be that we overestimate the involvement of the cortex in motor control, and that the cerebellum and other brain regions do most of the work, but it is also plausible that other regions function as associative hierarchical memories as well.

5. Spatial, Temporal, and beyond - We learn patterns in space, and in time, and in abstract systems that don't fit in time or space. A sufficiently general hierarchical associative memory should be able to apply a similar process to data of any type and, given sufficient time and memory, discover most of the patterns within it regardless of their dimensions. But in order to set off in the right direction, we must conceive of models that properly deal with time, which is apparently different in nature from other dimensions. Time is the one direction in which you must always go. Predictions about values in a spatial signal can be made out of order, immediately verified, and refined as needed. Predictions about values in a temporal signal can only be verified with limited certainty because they must necessarily be at least second-order predictions (predictions based on the results of other predictions). Uncertainty increases with distance from the present, as information about the past and future are inferences based on memory. My hope is that going forwards, machine learning researchers will begin to treat time as a special dimension, rather than just turning it into another spacial dimension by using sliding windows. If I had to guess, I'd say that the distance from the present could be reasonably aligned with the distance from the raw sensorimotor signals to create a single dimension of abstraction. I have a model in mind, but I suggest that this is not an answer to the question of how does the brain learn, but merely a general direction in which to look for answers.

These are steep requirements, but as I will describe, it's not impossible to stay within them and still develop computer models that work, And I believe that we must absolutely stay within them in order to make discoveries relevant to achieving human-level AI as soon as possible. In this thesis, I will be investigating the possibility of building a system that meets these requirements, based on the stacked de-noising auto-encoder, a type of deep belief network.



The synthesis of these ideas leads to the hypothesis that the cortex learns via a unified process of perception and control which continuously makes predictions that are desirable, parsimonious, and relevant to action. Observations in psychology such as the self-serving and confirmation cognitive biases support this view. We recall the world being closer to our own model than it really is, and our model is biased towards scenarios that would benefit us. This may an artifact of a beneficial adaptation: the brain's unification of perception and control in the neocortex. Under this hypothesis, we do not have a model of the world, and a set of goals from which we derive plans, as classical AI has sought to build. We have a single biased model which simultaneously represents both imperfectly. This model is hierarchically organized into levels of abstraction, and each level learns by roughly the same process. It is possible that the most abstract levels are more representative of goals than of the world, but each level is still permeated with bias to some extent. I elaborate on this model in Section~\ref{fart}. These ideas about how the brain might learn and perform perception and control seemed idealistic until Geoffrey Hinton's deep belief networks started showing successes in the field of machine vision. These unsupervised learning algorithms can build up levels of abstraction, and given enough computer power, can discover hierarchical latent structure in data. They seem like the perfect candidates for the scaffolding of a new model based on the ideas of all these scientists. 

\section{Relevant work}
 
	\subsection{Jeff Hawkins' ideas about the neocortex}
	
Jeff Hawkins, notable founder of Palm Computing, Handspring, and Numenta, has written extensively about his Memory-Prediction model of the mammalian neocortex. It can be summarized as follows: The Central Nervous System (CNS) consists of the cortex and lower brain structures. The lower brain structures produce movements by reflex with added noise. The cortex collects the signals sent out of the CNS and the sensory signals coming in from senses. It consists of sheet of functionally similar tissue, loosely subdivided into regions, each of which learns a predictive model of it's receptive field. Receptive fields can consist of sensory inflow, motor outflow, and predictions from other regions. Each region will actively predict the state of it's receptive field, passing those predictions down to those other regions in it's receptive field, and up, to any regions who's receptive field they are in. The global structure of the connections is a hierarchy, with large subtrees for each sense, and with an upper level where sensory integration occurs, containing mostly regions that predict abstract invariant representations.

% EXPAND

	\subsection{Daniel Wolpert's ideas about the motor-control purpose of the brain}
	
Daniel Wolpert studies sensorimotor integration and uses engineering methods to model how the CNS controls movement. He stresses that motor control is the adaptive purpose of the brain, because all of the brain's effects on the world are mediated through contractions of the muscles. Only organisms that move have brains, so it stands to reason that motion is closely connected to the brain's adaptive purpose. He and his team developed a model based on Kalman filters where the CNS makes simultaneous predictions of future sensory inputs and future motor outputs with the same process.

% EXPAND
	
	\subsection{Geoffrey Hinton's Recent successes with Deep Belief Networks}
	
Geoffrey Hinton, while he was at the University of Toronto, and recently, at Google, has been a pioneer in the field of Deep Belief Networks. These networks feature a layered network topology and methods for training them, that out-perform flat networks with an equal number of trainable parameters. Many types of learning modules can be stacked into a deep network. Hinton and his team have focused primarily on restricted Boltzmann machines (RBMs) and Stacked de-noising auto-encoders (SdAs)

% EXPAND

	\subsection{Bill Powers and Perceptual Control Theory}

Perceptual Control Theory is a formalized model of how an organism might control it's behavior. Specifically, It does not directly control it's behavior, it directly controls it's perceptions of the world. Some of the world can be indirectly influenced by it's muscles with varying levels of noise and delay, and via a complex mapping of patterns.

% EXPAND
		
\section{Background}
	\subsection{Deep belief nets}
	
The reason deep belief networks offer some benefit over a completely flat network with the same number of trainable parameters is that they can generalize at different levels and thus share more information. For example, a flat network which has learned a mapping between pictures of cars and labels cannot re-use any of those weights to learn a mapping between pictures of animals and labels. However, a deep network with an intermediate layer sensitive to edges could be shared between them and both classifiers could then be expressed with a far smaller number of weights, as all natural images can be more compactly represented as collections of edges. 

% EXPAND

	\subsection{Auto-encoders}
	
Autoencoders, or autoassociators as they are sometimes called, are a variant of the simple three-layer artificial neural network where the output is expected to equal the input and the hidden layer is smaller or sparser than than the input layer. % Split this up. What is a three layer artificial NN? Does the last clause refer to Autoencoders specifically?
An autoencoder takes an input $\mathbf x\in[0,1]^d$ and first maps it (with an encoder) % I'd take out that parenthetical or explain some of this language. What does hidden mean here?
to a hidden representation $\mathbf y\in[0,1]^{d'}$ through a deterministic mapping, e.g.:

\[
\mathbf y = s(\mathbf W\mathbf x + \mathbf b)
\]
Where $s$ is a non-linearity such as the sigmoid. % the sigmoid function?
The latent representation $\mathbf y$, or code is then mapped back (with a decoder) into a reconstruction $\mathbf z$ of same shape as $\mathbf x$  through a similar transformation, e.g.:
\[
\mathbf z = s(\mathbf W'\mathbf y + \mathbf b')
\]
$\mathbf z$ should be seen as a prediction, or {\it reconstruction}, of  $\mathbf x$ given the code  $\mathbf y$. The parameters of this model ($\mathbf W$, $\mathbf W'$, $\mathbf b$, and $\mathbf b'$) are optimized such that the average reconstruction error is minimized\cite{Bengio09}.

The aim of the autoencoder to learn the code $\mathbf y$ a distributed representation that captures the coordinates along the main factors of variation in the data (similarly to how principal component analysis (PCA) captures the main factors of variation in the data). Because $\mathbf y$ is viewed as a lossy compression of $\mathbf x$, it cannot be a good compression (with small loss) for all $\mathbf x$, so learning drives it to be one that is a good compression in particular for training examples, and hopefully for others as well, but not for arbitrary inputs. That is the sense in which an auto-encoder generalizes: it gives low reconstruction error to test examples from the same distribution as the training examples, but generally high reconstruction error to uniformly chosen configurations of the input vector.

If there is one linear hidden layer (the code) and the mean squared error criterion is used to train the network, then the  hidden units learn to project the input in the span of the first  principal components of the data. If the hidden layer is non-linear, the auto-encoder behaves differently from PCA, with the ability to capture multi-modal aspects of the input distribution. The departure from PCA becomes even more important when we consider stacking multiple encoders (and their corresponding decoders) when building a deep auto-encoder [Hinton06].

The auto-encoder alone is not sufficient to be the basis of a deep architecture because it has a tendency towards over-fitting. The denoising autoencoder (dA) is an extension of a classical autoencoder introduced specifically as a building block for deep networks\cite{Vincint08}.  It attempts to re-construct a corrupted version of the input, but the error in $\mathbf z$ is still compared against the un-corrupted input. The stochastic corruption process consists in randomly setting some of the inputs (as many as half of them) to zero. Hence the denoising auto-encoder is trying to predict the corrupted (i.e. missing) values from the uncorrupted (i.e., non-missing) values, for randomly selected subsets of missing patterns. This modification allows the dA to generalize well and produces compounding benefits when the dA's are stacked into a deep network\cite{Hinton06}. Hinton (google tech talk 3) suggests that the stochastic timing of the action potentials observed in biological neurons is a similar feature evolved to moderate the potential for over-fitting, and allow neurons or neuron groups to generalize well over the range of activation patterns of their receptive fields.

	\subsection{Stacked De-noising Auto-encoders}
	
Stacked denoising autoencoders, canonically abbreviated SdA, are not just neural networks with additional hidden layers, but a structure with individual levels of simple three-layer denoising autoencoders. First, a single denoising autoencoder is trained on the data. It's hidden layer converges on a sparse distributed representation of the training set. This essentially replaces the step where a researcher would have to design a collection of good features. Then, a second denoising autoencoder is trained to reconstruct corrupted versions of the activation of the hidden layer of the first dA for the collection of training examples. (the first level does not learn during this time). After a sufficient number of levels have been added, if the network is to be used for classification, the encoders and decoders from each level are assembled into one long network and fine-tuned using back-propagation.

% EXPAND

\section{Meeting the above criteria with stacked de-noising auto-encoders}
\label{fart}

It remains to be seen whether these deep learning algorithms can be applied successfully to motor control, high-level planning, and the many other functions we know the neocortex is involved in. Moving forward in the search for artificial general intelligence it is important that we test new algorithms on the same variety of tasks as we know the brain to be capable of solving, and that we test them at the same scale in realistic environments, to the extent that sufficient computer power is affordable. A scalable algorithm which has mediocre performance on a variety of tasks with today's computing power is more promising as a model of the brain than an algorithm that excels at only one task.

Classical AI has a neutral model of the world and a set of goals. Plans are derived from the combination of these two elements. I'm proposing that a successful biologically inspired learning algorithm could have only one model, biased towards desirable situations, which would represent the world, and the goals, simultaneously, and imperfectly. The benefit of this is it's simplicity, scalability, and self-similarity. In this model, a deep net would would be pre-trained on observations of the world and of one's own actions which are initially random. The system would then transition to a period of fine-tuning where desirable outcomes are enforced from the top down. Sensory information propagates up the network, and desirability-biased expectations propagate down the network. Each layer performs the same task, it modifies it's weights by a gradient based on a combination of data from below, and expectations from above. At the bottom, the data is ground truth, and any data about one's own actions becomes ground truth by taking those actions. At the top, the expectations are set explicitly by the programmer. 

If a modern deep belief network can be made to perform planning tasks, then I believe it lends strength to the biological plausibility of that algorithm. I am operating on the assumption for this project, that perception and planning are both part of the same process, so the most abstract representations are joint models of sensory experience and action. The model of sensory data is influenced by three main factors. The accuracy, the sparsity, and the desirability, or pleasurableness to the creature. At all levels, the representations which jointly satisfy these three metrics come to dominate. In the case of a game this last factor would be the points or wins/losses experienced by the player. The player perceives the state of the game and the state of it's own recent actions, and expects a likely, sparse, and desirable scenario next, and then is made to take any actions it expected of itself which are legal in the rules of the game. 

\section{Experimental Design}

	\subsection{Measuring the plasticity of an unsupervised learning algorithm.}
	
	In this thesis I set out to investigate one of the criteria I described above, plasticity. I will begin two kinds of data source, both represented as two dimensional squares in greyscale. The first, type is natural images, taken from ImageNet. The second is Discrete Fourier Transforms of speech audio snippets of the same spatial resolution. I start by training one SdA on each set, for the same number of epochs, and then test their reconstruction error. I then duplicate and swap the two networks, and train each one on the opposite dataset. this is to control for any order-sensitivity in the experiment. I then train these networks on the new data for a equal number of epochs and test their reconstruction error. un-swapped networks are also trained for another X epochs and tested. The SdA algorithm is deemed to display plasticity if the networks initialized with coherent but incorrect weights perform better than networks initialized with incorrect weights.
	
	\subsection{Implementation}
	
% EXPAND
		
	\subsection{Acquiring and preparing the data-set}
	
To construct the dataset, I first collected tens of thousands of recorded go games as SGF files from the IGS server and other sources such as professional tournaments. I then filtered out about 20 percent of those games because they were in incomplete or were played by nonstandard rules. Then each move in each games was recorded as a training example. Training examples were vectors of 363 numbers between 0 and 1. The first 361 numbers were the board positions of the 19x19 board. an empty position was represented with 0.5, while a white or black stone was represented with 0.75 or 0.25 respectively. The last moves of white and black were represented with 0 and 1 respectively. I chose not to represent the last moves as normalized coordinates because that would make them sensitive and would create a very non-smooth space for the most important parameters in the model. Finally, the last two numbers are the normalized clamped game move number and the final score for black. In each game, the moves of both players are used to create training examples but white's moves are inverted because all moves are stored from black's perspective and when the engine plays go, if it plays as white, it inverts the state to present it to the SdA.

Overall, 1.9 million training examples were created. This dataset was then divided randomly into training, test, and validation sets. 4/5ths of the initial set was used as training data, 4/5ths of what remained was test data, and the remaining part was the validation set. The dataset could not fit in main memory so it was randomized using parallel external merge sort of murmur3 hashes of the training example indices plus a constant seed offset. The randomized data were divided into 1 Gb chunks and stored on an SSD for fast loading into the GPU.
	
	\subsection{Greedy layer-wise pre-training}

The SdA was pre-trained on a dataset of 1,922,933 go positions from the color perspective of the winner. Only the moves in which the winner just places a stone were used. This is the sense in which the SdA preferentially remembers desirable situations, as opposed to remembering all situations in an unbiased model of reality and then deriving actions from the model and a goal. One attempt was made to modify the learning rate with respect to the game's final score on a point by point basis, but it was core-limited as it required communication between the CPU and GPU on every data point, and at the speed of one core, the training process would not have finished for several months. In order to fully take advantage of the GPU and finish in a reasonable amount of time, additional optimizations would have to be designed specifically for this application.

The network topology used was [363, 250, 150, 50, 10]. Each denoising autoencoder in the SdA was trained for 100 iterations of the entire dataset. The entire SdA has about $\mathbf 2^{18}$ trainable parameters.
	
	\subsection{Goal-oriented fine-tuning}

	
\section{Results}

	\subsection{Performance of twice-trained networks vs control group}

	\subsection{Analysis and visualization of hidden units}
	
\section{Discussion}

	\subsection{Potential Improvements to plasticity experiment}
	
	\subsection{Measuring other criteria}
	
	\subsection{Conclusion}



\begin{thebibliography}{99}
\singlespacing

\bibitem{Somebody09} asdasdasd

\end{thebibliography}
\end{document}