{\rtf1\ansi\ansicpg1252\cocoartf1038\cocoasubrtf360
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww28300\viewh17040\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\ql\qnatural\pardirnatural

\f0\fs30 \cf0 I was inspired by several researchers with ideas about the brain. Jeff Hawkins, who argues that although the neocortex has many specialized regions, that they all learn using roughly the same process, and Daniel Wolpert, who stresses that the evolutionary purpose of the brain is to produce complex adaptive movements. These scientists, and many others have been inching closer to a model which meets the many criteria for a plausible model of the neocortex. To be more specific, at this point, it appears we are looking for an online associative memory which can learn arbitrarily deep hierarchies of abstraction with the following criteria:\
\
1. Scalable - The model must be composed of a large number of similar units, which can be grown outward according to a simple pattern to increase the size of the total network. These units could be analogous to neurons, cortical columns, or some larger structure. This is desirable because it simplifies the process by which the neocortex could be grown in animals, and in the machine learning community, it is practical, easy to understand, and economical. We know that cortices of varying sizes occur in many mammals, and that the cortex provides some adaptive benefit at any size.\
\
2. Plastic - The model must be sufficiently general that it could be applied to video, audio, tactile, somatosensory, and other more abstract, data with minimal tuning of meta-parameters for each kind of data. To measure plasticity, one might train a model on some images, and then, train it on some audio and compare it's predictive performance to a randomly initialized model of equal size trained on the same audio for the same number of epochs.\
\
3. Online - The model must learn and predict continuously and incorporate new data as it becomes available. Most researchers prefer to train model on batches of data, and then test them, partly because offline algorithms are simpler, and it affords scientifically desirable properties like repeatability, but I think that the further down the road of offline learning algorithms we go, the more our results will be irrelevant to a plausible model of the neocortex. Often however, online learning algorithms are simpler, for example, the on-line and off-line versions of k-means.\
\
4. Capable of motor control - The neocortex is involved in motor control. It's not fully in charge of motor control, but it's definitely involved, and we must reconcile how an associative memory could be part of a motor control and planning system. Can motor control be formulated as a categorically similar problem to perception? Such as in Bill Power's Perceptual Control Theory which hypothesizes that perceptions are controlled rather than actions, and that action is the result of negative feedback systems bringing the world into agreement with perception. I suggest that motor control is somehow unified with perception, such that the same process which can abstract faces and cars from lines and spots in visual cortex, could abstract waving or waterskiing from postures and cycles of motion. It could presumably use the same information pathways by which predictions are propagated downward towards the senses, to decode actions into muscle contractions. However, it may be that we overestimate the involvement of the cortex in motor control, and that the cerebellum and other brain regions do most of the work, but it is also plausible that other regions function as associative hierarchical memories as well.\
\
5. Spatial, Temporal, and beyond - We learn patterns in space, and in time, and in abstract systems that don't fit in time or space. A sufficiently general hierarchical associative memory should be able to apply a similar process to data of any type and, given sufficient time and memory, discover most of the patterns within it regardless of their dimensions. But in order to set off in the right direction, we must conceive of models that properly deal with time, which is apparently different in nature from other dimensions. Time is the one direction in which you must always go. Predictions about values in a spatial signal can be made out of order, immediately verified, and refined as needed. Predictions about values in a temporal signal can only be verified with limited certainty because they must necessarily be at least second-order predictions (predictions based on the results of other predictions). Uncertainty increases with distance from the present, as information about the past and future are inferences based on memory. My hope is that going forwards, machine learning researchers will begin to treat time as a special dimension, rather than just turning it into another spacial dimension by using sliding windows. If I had to guess, I'd say that the distance from the present could be reasonably aligned with the distance from the raw sensorimotor signals to create a single dimension of abstraction. I have a model in mind, but I suggest that this is not an answer to the question of how does the brain learn, but merely a general direction in which to look for answers.\
\
These are steep requirements, but as I will describe, it's not impossible to stay within them and still develop computer models that work, And I believe that we must absolutely stay within them in order to make discoveries relevant to achieving human-level AI as soon as possible. In this thesis, I will be investigating the possibility of building a system that meets these requirements, based on the stacked de-noising auto-encoder, a type of deep belief network.}